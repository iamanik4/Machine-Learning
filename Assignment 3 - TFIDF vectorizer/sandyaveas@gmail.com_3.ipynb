{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating TF-IDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "\n",
    "def fit(dataset, top_50_idf = False):\n",
    "    if isinstance(dataset, (list,)):\n",
    "        unique_words = set()\n",
    "        for row in dataset:\n",
    "            for word in row.split(' '):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        #features = unique_words\n",
    "        \n",
    "        #Calculating idf of features\n",
    "        dict_IDF = calculateIDF(dataset, vocab)\n",
    "\n",
    "        #Sorting IDF in reverse and vocab as per the top 50 idf values\n",
    "        if(top_50_idf):\n",
    "            dict_IDF = dict(sorted(dict_IDF.items(), key=lambda kv: kv[1], reverse=True)[:50])\n",
    "            \n",
    "            vocab = {word : idx for idx ,word in enumerate(dict_IDF.keys())}\n",
    "\n",
    "        return vocab, dict_IDF\n",
    "    else:\n",
    "        print(\"you need to pass list of sentence\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculateTF(word_frequency, document_len):\n",
    "    return word_frequency/document_len\n",
    "\n",
    "\n",
    "def calculateIDF(dataset, vocab):\n",
    "\n",
    "    corpus_len = len(dataset)    \n",
    "\n",
    "    word_in_no_of_doc = {word : 0 for word, frequency in vocab.items()}\n",
    "    \n",
    "    for document in dataset:\n",
    "        for word in vocab:\n",
    "            if word in document:\n",
    "                word_in_no_of_doc[word] = int(word_in_no_of_doc[word]) + 1\n",
    "    \n",
    "    \n",
    "    dict_IDF = word_in_no_of_doc\n",
    "    \n",
    "    for word, occurences in word_in_no_of_doc.items():\n",
    "        dict_IDF[word] = 1 + math.log( (1+ corpus_len )/ (1 + occurences))\n",
    "        \n",
    "\n",
    "    return dict_IDF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform(dataset, vocab, dict_IDF):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    \n",
    "    if isinstance(dataset, (list,)):        \n",
    "        \n",
    "        for idx, row in enumerate(tqdm(dataset)):\n",
    "            word_freq = dict(Counter(row.split()))            \n",
    "            \n",
    "            document_len = len(row)\n",
    "            \n",
    "            for word in row.split():\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                \n",
    "                col_index = vocab.get(word, -1)\n",
    "                \n",
    "                if col_index !=-1:                    \n",
    "                    rows.append(idx)                    \n",
    "                    columns.append(col_index)\n",
    "                    \n",
    "                    tf = calculateTF(word_freq[word], document_len)                   \n",
    "                    idf = dict_IDF[word]\n",
    "                    \n",
    "                    \n",
    "                    tfidf = tf * idf\n",
    "                    \n",
    "                    values.append(tfidf)\n",
    "                \n",
    "        csr_mat = csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "\n",
    "        normlized_matrix = normalize(csr_mat, norm='l2')\n",
    "\n",
    "\n",
    "        return normlized_matrix\n",
    "\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 3996.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.46979138557992045\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "  (1, 1)\t0.8843203931656719\n",
      "  (1, 3)\t0.18074746668441158\n",
      "  (1, 5)\t0.34636469521707053\n",
      "  (1, 6)\t0.18074746668441158\n",
      "  (1, 8)\t0.18074746668441158\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.46979138557992045\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 3)\t0.3840852409148149\n",
      "  (3, 6)\t0.3840852409148149\n",
      "  (3, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "\n",
    "vocab, dictIDF = fit(corpus)\n",
    "print(list(vocab.keys()))\n",
    "\n",
    "tfidf = transform(corpus, vocab, dictIDF)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aailiyah', 'abandoned', 'abroad', 'abstruse', 'academy', 'accents', 'accessible', 'acclaimed', 'accolades', 'accurately', 'achille', 'ackerman', 'adams', 'added', 'admins', 'admiration', 'admitted', 'adrift', 'adventure', 'aesthetically', 'affected', 'affleck', 'afternoon', 'agreed', 'aimless', 'aired', 'akasha', 'alert', 'alike', 'allison', 'allowing', 'alongside', 'amateurish', 'amazed', 'amazingly', 'amusing', 'amust', 'anatomist', 'angela', 'angelina', 'angry', 'anguish', 'angus', 'animals', 'animated', 'anita', 'anniversary', 'anthony', 'antithesis', 'anyway']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 746/746 [00:00<00:00, 74667.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 24)\t1.0\n",
      "  (19, 43)\t1.0\n",
      "  (68, 21)\t1.0\n",
      "  (72, 23)\t1.0\n",
      "  (74, 25)\t1.0\n",
      "  (89, 47)\t1.0\n",
      "  (135, 3)\t0.3779644730092272\n",
      "  (135, 9)\t0.3779644730092272\n",
      "  (135, 15)\t0.3779644730092272\n",
      "  (135, 17)\t0.3779644730092272\n",
      "  (135, 29)\t0.3779644730092272\n",
      "  (135, 32)\t0.3779644730092272\n",
      "  (135, 40)\t0.3779644730092272\n",
      "  (176, 39)\t1.0\n",
      "  (192, 18)\t1.0\n",
      "  (193, 20)\t1.0\n",
      "  (216, 2)\t1.0\n",
      "  (225, 16)\t1.0\n",
      "  (227, 14)\t1.0\n",
      "  (241, 35)\t1.0\n",
      "  (270, 1)\t1.0\n",
      "  (290, 22)\t1.0\n",
      "  (341, 34)\t1.0\n",
      "  (344, 33)\t1.0\n",
      "  (348, 8)\t1.0\n",
      "  (409, 5)\t1.0\n",
      "  (430, 31)\t1.0\n",
      "  (457, 36)\t1.0\n",
      "  (461, 4)\t0.7071067811865476\n",
      "  (461, 44)\t0.7071067811865476\n",
      "  (465, 30)\t1.0\n",
      "  (475, 28)\t1.0\n",
      "  (493, 6)\t1.0\n",
      "  (500, 38)\t1.0\n",
      "  (544, 41)\t1.0\n",
      "  (548, 0)\t0.7071067811865476\n",
      "  (548, 26)\t0.7071067811865476\n",
      "  (608, 12)\t1.0\n",
      "  (612, 10)\t1.0\n",
      "  (620, 37)\t0.7071067811865476\n",
      "  (620, 42)\t0.7071067811865476\n",
      "  (632, 7)\t1.0\n",
      "  (644, 11)\t0.5773502691896258\n",
      "  (644, 45)\t0.5773502691896258\n",
      "  (644, 46)\t0.5773502691896258\n",
      "  (667, 19)\t1.0\n",
      "  (691, 27)\t1.0\n",
      "  (699, 48)\t1.0\n",
      "  (722, 13)\t1.0\n",
      "  (735, 49)\t1.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "\n",
    "vocab, dict_IDF = fit(corpus, True)\n",
    "print(list(vocab.keys()))\n",
    "\n",
    "tfidf = transform(corpus, vocab, dict_IDF)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please give feedback on this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
