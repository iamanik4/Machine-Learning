{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLibrary:\n",
    "    \n",
    "    #-------------Common code starts from here for Part A and B------------#\n",
    "    def convert_DataFrame_Proba_by_threshold(self, df, column_name, threshold):\n",
    "        \n",
    "        df_with_y_hat = df.copy()\n",
    "        df_with_y_hat.loc[df_with_y_hat[column_name] >= threshold, column_name] = 1\n",
    "        df_with_y_hat.loc[df_with_y_hat[column_name] < threshold, column_name] = 0\n",
    "        \n",
    "        df_with_y_hat = df_with_y_hat.rename(columns={column_name:'y_hat'})\n",
    "        \n",
    "        return df_with_y_hat\n",
    "    \n",
    "    #A.1 and B.1\n",
    "    def getConfusionMatrixValues(self, df):\n",
    "        \n",
    "        true_positive = len(df[((df.y == 1) & (df.y_hat == 1))])\n",
    "        \n",
    "        false_positive = len(df[((df.y == 0) & (df.y_hat == 1))])\n",
    "        \n",
    "        false_negative = len(df[((df.y == 1) & (df.y_hat == 0))])\n",
    "        \n",
    "        true_negative = len(df[((df.y == 0) & (df.y_hat == 0))])\n",
    "        \n",
    "        return true_positive, false_positive, false_negative, true_negative   \n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_Precision(self, true_positive, false_positive):\n",
    "        \n",
    "        if (true_positive + false_positive) > 0:\n",
    "            return (true_positive / (true_positive + false_positive))\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def compute_Recall(self, true_positive, false_negative):\n",
    "        \n",
    "        if (true_positive + false_negative) > 0:\n",
    "            return (true_positive / (true_positive + false_negative))\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    #A.2 and B.2\n",
    "    def compute_F1_Score(self, precision, recall):\n",
    "        \n",
    "        if (precision + recall) > 0:\n",
    "            return (2 * precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    #A.4 and B.4\n",
    "    def compute_Accuracy_Score(self, true_positive, true_negative, tot_num_points):\n",
    "        return (true_positive + true_negative) / tot_num_points\n",
    "        \n",
    "        \n",
    "    def compute_TruePositiveRate(self, true_positive, false_negative):\n",
    "        return self.compute_Recall(true_positive, false_negative)\n",
    "    \n",
    "    \n",
    "    def compute_FalsePositiveRate(self, false_positive, true_negative):\n",
    "        \n",
    "        if (false_positive + true_negative) > 0:\n",
    "            return (false_positive / (false_positive + true_negative))\n",
    "        else:\n",
    "            return 0\n",
    "         \n",
    "    \n",
    "    \n",
    "    #A.3 and B.3\n",
    "    def compute_AUC_Score(self, df):\n",
    "        unique_proba = sorted(df.proba.unique(), reverse=True)\n",
    "        \n",
    "        tpr_array = []\n",
    "        fpr_array = []\n",
    "        for threshold in unique_proba:\n",
    "            \n",
    "            df_with_y_hat = self.convert_DataFrame_Proba_by_threshold(df, 'proba', threshold)\n",
    "            \n",
    "            t_p, f_p, f_n, t_n = self.getConfusionMatrixValues(df_with_y_hat)\n",
    "            \n",
    "            tpr_array.append(self.compute_TruePositiveRate(t_p, f_n))\n",
    "            \n",
    "            fpr_array.append(self.compute_FalsePositiveRate(f_p, t_n))           \n",
    "\n",
    "        return np.trapz(tpr_array, fpr_array)\n",
    "    #-------------Common code ends here for Part A and B------------#\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #-------------Part C------------#\n",
    "    def compute_best_threshold(self, df):\n",
    "        \n",
    "        unique_prob = sorted(df.prob.unique(), reverse = True)\n",
    "        \n",
    "        dict_A = {}\n",
    "        for threshold in unique_prob:\n",
    "            \n",
    "            df_with_y_hat = self.convert_DataFrame_Proba_by_threshold(df, 'prob', threshold)\n",
    "            \n",
    "            t_p, f_p, f_n, t_n = self.getConfusionMatrixValues(df_with_y_hat)\n",
    "            \n",
    "            A = (500 * f_n) + (100 * f_p)\n",
    "            \n",
    "            dict_A[threshold] = A\n",
    "        \n",
    "        \n",
    "        best_threshold = min(dict_A, key=dict_A.get)\n",
    "\n",
    "        return best_threshold, dict_A[best_threshold]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-------------Part D------------#\n",
    "    #D.1\n",
    "    def compute_Mean_Square_Error(self, df):\n",
    "        \n",
    "        n = len(df)\n",
    "        \n",
    "        summation_value = 0\n",
    "        for index, row in df.iterrows():\n",
    "            summation_value += pow(float(row[\"y\"]) - float(row[\"pred\"]), 2)\n",
    "            \n",
    "        return summation_value / n\n",
    "    \n",
    "    #D.2\n",
    "    def compute_mean_absolute_percentage_error(self, df):\n",
    "        \n",
    "        summation_error = 0\n",
    "        summation_actual = df.y.sum()\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            summation_error += abs(row[\"y\"] - row[\"pred\"])\n",
    "        \n",
    "        return summation_error / summation_actual\n",
    "    \n",
    "    #D.3\n",
    "    def compute_R_Square(self, df):\n",
    "        \n",
    "        mean_y_pred = df.pred.mean()\n",
    "        \n",
    "        SS_Tot = 0\n",
    "        SS_Res = 0        \n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            SS_Tot += pow((row[\"y\"] - mean_y_pred), 2)\n",
    "            SS_Res += pow((row[\"y\"] - row[\"pred\"]), 2)\n",
    "            \n",
    "            \n",
    "        return (1 - (SS_Res/SS_Tot))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the class object\n",
    "custom_Lib = customLibrary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Confusion Matrix-------\n",
      "[[10000, 100], [0, 0]]\n",
      "\n",
      "-------F1 Score-------\n",
      "0.9950248756218906\n",
      "\n",
      "-------Accuracy Score-------\n",
      "0.9900990099009901\n",
      "\n",
      "-------AUC Score-------\n",
      "0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('5_a.csv')\n",
    "\n",
    "df_with_y_hat = custom_Lib.convert_DataFrame_Proba_by_threshold(df, 'proba', 0.5)\n",
    "\n",
    "t_p, f_p, f_n, t_n = custom_Lib.getConfusionMatrixValues(df_with_y_hat)\n",
    "\n",
    "confusion_matrix = [[t_p, f_p], [f_n, t_n]]\n",
    "\n",
    "precision = custom_Lib.compute_Precision(t_p, f_p)\n",
    "recall = custom_Lib.compute_Recall(t_p, f_n)\n",
    "f1_score = custom_Lib.compute_F1_Score(precision, recall)\n",
    "acc_score = custom_Lib.compute_Accuracy_Score(t_p, t_n, len(df))\n",
    "auc_score = custom_Lib.compute_AUC_Score(df)\n",
    "\n",
    "print('\\n-------Confusion Matrix-------')\n",
    "print(confusion_matrix)\n",
    "\n",
    "print('\\n-------F1 Score-------')\n",
    "print(f1_score)\n",
    "\n",
    "\n",
    "print('\\n-------Accuracy Score-------')\n",
    "print(acc_score)\n",
    "\n",
    "print('\\n-------AUC Score-------')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Confusion Matrix-------\n",
      "[[55, 239], [45, 9761]]\n",
      "\n",
      "-------F1 Score-------\n",
      "0.2791878172588833\n",
      "\n",
      "-------Accuracy Score-------\n",
      "0.9718811881188119\n",
      "\n",
      "-------AUC Score-------\n",
      "0.9377570000000001\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('5_b.csv')\n",
    "\n",
    "df_with_y_hat = custom_Lib.convert_DataFrame_Proba_by_threshold(df, 'proba', 0.5)\n",
    "\n",
    "t_p, f_p, f_n, t_n = custom_Lib.getConfusionMatrixValues(df_with_y_hat)\n",
    "\n",
    "confusion_matrix = [[t_p, f_p], [f_n, t_n]]\n",
    "\n",
    "precision = custom_Lib.compute_Precision(t_p, f_p)\n",
    "recall = custom_Lib.compute_Recall(t_p, f_n)\n",
    "f1_score = custom_Lib.compute_F1_Score(precision, recall)\n",
    "acc_score = custom_Lib.compute_Accuracy_Score(t_p, t_n, len(df))\n",
    "auc_score = custom_Lib.compute_AUC_Score(df)\n",
    "\n",
    "print('\\n-------Confusion Matrix-------')\n",
    "print(confusion_matrix)\n",
    "\n",
    "print('\\n-------F1 Score-------')\n",
    "print(f1_score)\n",
    "\n",
    "\n",
    "print('\\n-------Accuracy Score-------')\n",
    "print(acc_score)\n",
    "\n",
    "print('\\n-------AUC Score-------')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Lowest Metric-------\n",
      "141000\n",
      "\n",
      "-------Best Threshold-------\n",
      "0.2300390278970873\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('5_c.csv')\n",
    "\n",
    "best_threshold, lowest_metric = custom_Lib.compute_best_threshold(df)\n",
    "\n",
    "print('\\n-------Lowest Metric-------')\n",
    "print(lowest_metric)\n",
    "\n",
    "print('\\n-------Best Threshold-------')\n",
    "print(best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------MSE-------\n",
      "177.16569974554707\n",
      "\n",
      "-------MAPE-------\n",
      "0.1291202994009687\n",
      "\n",
      "-------R Square-------\n",
      "0.9563583447288628\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('5_d.csv')\n",
    "\n",
    "mse = custom_Lib.compute_Mean_Square_Error(df)\n",
    "\n",
    "mape = custom_Lib.compute_mean_absolute_percentage_error(df)\n",
    "\n",
    "r_square = custom_Lib.compute_R_Square(df)\n",
    "\n",
    "print('\\n-------MSE-------')\n",
    "print(mse)\n",
    "\n",
    "print('\\n-------MAPE-------')\n",
    "print(mape)\n",
    "\n",
    "print('\\n-------R Square-------')\n",
    "print(r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please give the feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
